{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification SemEVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling,margin_sampling,entropy_sampling\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised_learner():\n",
    "    def __init__(self,X,Y,sp_model):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.model = sp_model\n",
    "\n",
    "    def learn(self):\n",
    "        X_train,X_test,Y_train,Y_test = train_test_split(self.X,self.Y,test_size = 0.2)\n",
    "        model = self.model\n",
    "        model.fit(X_train, Y_train)\n",
    "        model_accuracy = model.score(X_test, Y_test)\n",
    "        model_pred = model.predict(X_test)\n",
    "        model_f1 = f1_score(Y_test,model_pred,average =\"weighted\")\n",
    "        return model_accuracy,model_f1,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Active_learner():\n",
    "    def __init__(self,X,Y,model,data, percentage,query_method):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.short_df = data.copy()\n",
    "        self.percent = percentage\n",
    "        self.model = model\n",
    "        self.query_method = query_method\n",
    "        \n",
    "    def learn(self):       \n",
    "        # seeding\n",
    "        classes = self.short_df['grades_round'].unique()\n",
    "        seed_index = []\n",
    "        for i in classes:\n",
    "            seed_index.append(self.short_df['grades_round'][self.short_df['grades_round']==i].index[0])\n",
    "        seed_index\n",
    "\n",
    "        act_data = self.short_df.copy()\n",
    "        accuracy_list = []\n",
    "\n",
    "        # initialising\n",
    "        train_idx = seed_index\n",
    "        X_train = self.X[train_idx]\n",
    "        y_train = self.Y[train_idx]\n",
    "\n",
    "        # generating the pool\n",
    "        X_pool = np.delete(self.X, train_idx, axis=0)\n",
    "        y_pool = np.delete(self.Y, train_idx)\n",
    "\n",
    "        act_data = act_data.drop(axis=0,index = train_idx)\n",
    "        act_data.reset_index(drop = True,inplace=True)\n",
    "\n",
    "\n",
    "        # initializing the active learner\n",
    "\n",
    "        learner = ActiveLearner(\n",
    "            estimator = self.model,\n",
    "            X_training = X_train, y_training=y_train,\n",
    "            query_strategy=self.query_method\n",
    "        )\n",
    "\n",
    "        # pool-based sampling\n",
    "        n_queries = int(len(X)/(100/self.percent))\n",
    "        for idx in range(n_queries):\n",
    "            query_idx, query_instance = learner.query(X_pool)   \n",
    "            learner.teach(\n",
    "                X=X_pool[query_idx].reshape(1, -1),\n",
    "                y=y_pool[query_idx].reshape(1, )\n",
    "            )\n",
    "\n",
    "            # remove queried instance from pool\n",
    "            X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "            y_pool = np.delete(y_pool, query_idx)\n",
    "\n",
    "            act_data = act_data.drop(axis=0,index = query_idx)\n",
    "            act_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            accuracy_list.append(learner.score(X_pool,y_pool))\n",
    "#             print('Accuracy after query no. %d: %f' % (idx+1, learner.score(X_pool, y_pool)))\n",
    "        print(\"By just labelling \",round(n_queries*100.0/len(X),2),\"% of total data accuracy of \", round(learner.score(X_pool, y_pool),3), \" % is achieved on the unseen data\" )\n",
    "        learner_pred = learner.predict(X_pool)\n",
    "        learner_f1 = f1_score(y_pool,learner_pred,average='weighted')\n",
    "        return accuracy_list,learner_f1,learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sultan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating inputs and labels for sultan\n",
    "X = df[['length_ratio','aligned_score','aligned_score_demo','cos_similarity','cos_similarity_demo']]\n",
    "X = np.array(X)\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = []\n",
    "for i,model in enumerate(models):\n",
    "    ac = Active_learner(X,Y,model,df, 30)\n",
    "    accuracy_list,f1 = ac.learn()\n",
    "    dict_accuracy_al[i] = accuracy_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    \n",
    "dict_accuracy_sl= []    \n",
    "#Logistic regression\n",
    "lr_accuracy_list = []\n",
    "lr_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, Y_train)\n",
    "    lr_accuracy = lr.score(X_test, Y_test)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_f1 = f1_score(Y_test,lr_pred,average='weighted')\n",
    "    lr_accuracy_list.append(lr_accuracy)\n",
    "    lr_f1_list.append(lr_f1)\n",
    "lr_accuracy_mean = np.mean(lr_accuracy_list)\n",
    "lr_f1_mean = np.mean(lr_f1_list)\n",
    "dict_accuracy_sl.append(lr_accuracy_mean)\n",
    "\n",
    "# naive bayes classfier    \n",
    "nb_accuracy_list = []\n",
    "nb_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, Y_train)\n",
    "    nb_accuracy = nb.score(X_test, Y_test)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_f1 = f1_score(Y_test,nb_pred,average='weighted')\n",
    "    nb_accuracy_list.append(nb_accuracy)\n",
    "    nb_f1_list.append(nb_f1)\n",
    "nb_accuracy_mean = np.mean(nb_accuracy_list)\n",
    "nb_f1_mean = np.mean(nb_f1_list)\n",
    "dict_accuracy_sl.append(nb_accuracy_mean)\n",
    "\n",
    "\n",
    "\n",
    "# randomforest classfier    \n",
    "rf_accuracy_list = []\n",
    "rf_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, Y_train)\n",
    "    rf_accuracy = rf.score(X_test, Y_test)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_f1 = f1_score(Y_test,rf_pred,average='weighted')\n",
    "    rf_accuracy_list.append(rf_accuracy)\n",
    "    rf_f1_list.append(rf_f1)\n",
    "rf_accuracy_mean = np.mean(rf_accuracy_list)\n",
    "rf_f1_mean = np.mean(rf_f1_list)\n",
    "dict_accuracy_sl.append(rf_accuracy_mean)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC classfier    \n",
    "lsvc_accuracy_list = []\n",
    "lsvc_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    lsvc = SVC(kernel='linear',probability=True)\n",
    "    lsvc.fit(X_train, Y_train)\n",
    "    lsvc_accuracy = lsvc.score(X_test, Y_test)\n",
    "    lsvc_pred = lsvc.predict(X_test)\n",
    "    lsvc_f1 = f1_score(Y_test,lsvc_pred,average='weighted')\n",
    "    lsvc_accuracy_list.append(lsvc_accuracy)\n",
    "    lsvc_f1_list.append(lsvc_f1)\n",
    "lsvc_accuracy_mean = np.mean(lsvc_accuracy_list)\n",
    "lsvc_f1_mean = np.mean(lsvc_f1_list)\n",
    "dict_accuracy_sl.append(lsvc_accuracy_mean)\n",
    "\n",
    "\n",
    "# RBF_SVC   \n",
    "rsvc_accuracy_list = []\n",
    "rsvc_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    rsvc = SVC(probability=True)\n",
    "    rsvc.fit(X_train, Y_train)\n",
    "    rsvc_accuracy = rsvc.score(X_test, Y_test)\n",
    "    rsvc_pred = rsvc.predict(X_test)\n",
    "    rsvc_f1 = f1_score(Y_test,rsvc_pred,average='weighted')\n",
    "    rsvc_accuracy_list.append(rsvc_accuracy)\n",
    "    rsvc_f1_list.append(rsvc_f1)\n",
    "rsvc_accuracy_mean = np.mean(rsvc_accuracy_list)\n",
    "rsvc_f1_mean = np.mean(rsvc_f1_list)\n",
    "dict_accuracy_sl.append(rsvc_accuracy_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,20))\n",
    "\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0])-1, len(dict_accuracy_al[0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0]))])\n",
    "    ax.plot(dict_accuracy_al[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating inputs and labels for BOW\n",
    "CV = CountVectorizer()\n",
    "student_answer_count_vector = CV.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "iteration_count = 2\n",
    "Percent = 30\n",
    "\n",
    "\n",
    "#test\n",
    "test1df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test1.pkl\")\n",
    "student_answer_count_vector = CV.transform(test1df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test1X = student_answer_count_vector\n",
    "test1Y = test1df['grades_round'].values\n",
    "\n",
    "test2df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test2.pkl\")\n",
    "student_answer_count_vector = CV.transform(test2df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test2X = student_answer_count_vector\n",
    "test2Y = test2df['grades_round'].values\n",
    "\n",
    "test3df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test3.pkl\")\n",
    "student_answer_count_vector = CV.transform(test3df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test3X = student_answer_count_vector\n",
    "test3Y = test3df['grades_round'].values\n",
    "\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "print(test1X.shape,test1Y.shape)\n",
    "print(test2X.shape,test2Y.shape)\n",
    "print(test3X.shape,test3Y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Active learner\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "query_methods = [uncertainty_sampling,margin_sampling,entropy_sampling]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = {}\n",
    "t1_score_list = {}\n",
    "t2_score_list = {}\n",
    "t3_score_list = {}\n",
    "for i,model in enumerate(models):\n",
    "    dict_accuracy_al[i] = []\n",
    "    f1_score_list[i] = []\n",
    "    t1_score_list[i] = []\n",
    "    t2_score_list[i] = []\n",
    "    t3_score_list[i] = []\n",
    "    print(\"******************************************************************************\")\n",
    "    for query_method in query_methods:\n",
    "        ac = Active_learner(X,Y,model,df, Percent,query_method)\n",
    "        accuracy_list,f1,model_al = ac.learn()\n",
    "        dict_accuracy_al[i].append(accuracy_list)\n",
    "        f1_score_list[i].append(f1_score)\n",
    "        t1_score_list[i].append(model_al.score(test1X,test1Y))\n",
    "        t2_score_list[i].append(model_al.score(test2X,test2Y))\n",
    "        t3_score_list[i].append(model_al.score(test3X,test3Y))\n",
    "\n",
    "        \n",
    "## Supervised learner\n",
    "dict_accuracy_sl= []\n",
    "dict_f1_score_sl= []\n",
    "test1_sl = []\n",
    "test2_sl = []\n",
    "test3_sl = []\n",
    "\n",
    "for model in models:\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    test3 = []\n",
    "    for _ in range(iteration_count):\n",
    "        sl = Supervised_learner(X,Y,model)\n",
    "        accuracy,f1score,model_sl = sl.learn()\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1score)\n",
    "        test1.append(model_sl.score(test1X,test1Y))\n",
    "        test2.append(model_sl.score(test2X,test2Y))\n",
    "        test3.append(model_sl.score(test3X,test3Y))\n",
    "        \n",
    "        \n",
    "    dict_accuracy_sl.append(np.mean(accuracy_list))\n",
    "    dict_f1_score_sl.append(np.mean(f1_list))    \n",
    "    test1_sl.append(np.mean(test1))\n",
    "    test2_sl.append(np.mean(test2))\n",
    "    test3_sl.append(np.mean(test3))\n",
    "    \n",
    "    \n",
    "## plotting\n",
    "fig = plt.figure(figsize=(18,20))\n",
    "model_title = [\"Logistic regression\",\"Naive Bayes\",\"Random Forest\",\"SVC-linear\",\"SVC\"]\n",
    "query_strategy = [\"uncertainty_sampling\",\"margin_sampling\",\"entropy_sampling\"]\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0][0])-1, len(dict_accuracy_al[0][0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0][0]))],label = \"Supervised learner\")\n",
    "    for j in range(len(query_strategy)):\n",
    "        ax.plot(dict_accuracy_al[i][j],label = \"Active learner {}\".format(query_strategy[j]))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel(\"Number of queries\")\n",
    "    ax.set_ylabel(\"Accuracy %\")\n",
    "    ax.set_title(model_title[i])\n",
    "    ax.grid(color='g', linestyle='-', linewidth=0.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## storing the pikle\n",
    "import pickle as pkl\n",
    "dict_accuracy_al, f1_score_list , dict_accuracy_sl , dict_f1_score_sl\n",
    "pkl.dump( dict_accuracy_al, open( \"../../results/sem_bag_dict_accuracy_al.p\", \"wb\" ) )\n",
    "pkl.dump( f1_score_list, open( \"../../results/sem_bag_tfidf_score_list.p\", \"wb\" ) )\n",
    "pkl.dump( dict_accuracy_sl, open( \"../../results/sem_bag_dict_accuracy_sl.p\", \"wb\" ) )\n",
    "pkl.dump( dict_f1_score_sl, open( \"../../results/sem_bag_dict_f1_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( t1_score_list, open( \"../../results/sem_bag_dict_t1_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( t2_score_list, open( \"../../results/sem_bag_dict_t2_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( t3_score_list, open( \"../../results/sem_bag_dict_t3_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( test1_sl, open( \"../../results/sem_bag_dict_t1_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( test2_sl, open( \"../../results/sem_bag_dict_t2_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( test3_sl, open( \"../../results/sem_bag_dict_t3_score_sl.p\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-IDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating inputs and labels for TFidf\n",
    "Tf = TfidfVectorizer()\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "iteration_count = 2\n",
    "Percent = 30\n",
    "\n",
    "#test\n",
    "test1df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test1.pkl\")\n",
    "student_answer_count_vector = Tf.transform(test1df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test1X = student_answer_count_vector\n",
    "test1Y = test1df['grades_round'].values\n",
    "test2df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test2.pkl\")\n",
    "student_answer_count_vector = Tf.transform(test2df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test2X = student_answer_count_vector\n",
    "test2Y = test2df['grades_round'].values\n",
    "test3df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test3.pkl\")\n",
    "student_answer_count_vector = Tf.transform(test3df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test3X = student_answer_count_vector\n",
    "test3Y = test3df['grades_round'].values\n",
    "\n",
    "\n",
    "## Active learner\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "query_methods = [uncertainty_sampling,margin_sampling,entropy_sampling]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = {}\n",
    "t1_score_list = {}\n",
    "t2_score_list = {}\n",
    "t3_score_list = {}\n",
    "for i,model in enumerate(models):\n",
    "    dict_accuracy_al[i] = []\n",
    "    f1_score_list[i] = []\n",
    "    t1_score_list[i] = []\n",
    "    t2_score_list[i] = []\n",
    "    t3_score_list[i] = []\n",
    "    print(\"******************************************************************************\")\n",
    "    for query_method in query_methods:\n",
    "        ac = Active_learner(X,Y,model,df, Percent,query_method)\n",
    "        accuracy_list,f1,model_al = ac.learn()\n",
    "        dict_accuracy_al[i].append(accuracy_list)\n",
    "        f1_score_list[i].append(f1_score)\n",
    "        t1_score_list[i].append(model_al.score(test1X,test1Y))\n",
    "        t2_score_list[i].append(model_al.score(test2X,test2Y))\n",
    "        t3_score_list[i].append(model_al.score(test3X,test3Y))\n",
    "\n",
    "        \n",
    "## Supervised learner\n",
    "dict_accuracy_sl= []\n",
    "dict_f1_score_sl= []\n",
    "test1_sl = []\n",
    "test2_sl = []\n",
    "test3_sl = []\n",
    "\n",
    "for model in models:\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    test3 = []\n",
    "    for _ in range(iteration_count):\n",
    "        sl = Supervised_learner(X,Y,model)\n",
    "        accuracy,f1score,model_sl = sl.learn()\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1score)\n",
    "        test1.append(model_sl.score(test1X,test1Y))\n",
    "        test2.append(model_sl.score(test2X,test2Y))\n",
    "        test3.append(model_sl.score(test3X,test3Y))\n",
    "        \n",
    "        \n",
    "    dict_accuracy_sl.append(np.mean(accuracy_list))\n",
    "    dict_f1_score_sl.append(np.mean(f1_list))    \n",
    "    test1_sl.append(np.mean(test1))\n",
    "    test2_sl.append(np.mean(test2))\n",
    "    test3_sl.append(np.mean(test3))\n",
    "    \n",
    "    \n",
    "## plotting\n",
    "fig = plt.figure(figsize=(18,20))\n",
    "model_title = [\"Logistic regression\",\"Naive Bayes\",\"Random Forest\",\"SVC-linear\",\"SVC\"]\n",
    "query_strategy = [\"uncertainty_sampling\",\"margin_sampling\",\"entropy_sampling\"]\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0][0])-1, len(dict_accuracy_al[0][0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0][0]))],label = \"Supervised learner\")\n",
    "    for j in range(len(query_strategy)):\n",
    "        ax.plot(dict_accuracy_al[i][j],label = \"Active learner {}\".format(query_strategy[j]))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel(\"Number of queries\")\n",
    "    ax.set_ylabel(\"Accuracy %\")\n",
    "    ax.set_title(model_title[i])\n",
    "    ax.grid(color='g', linestyle='-', linewidth=0.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## storing the pikle\n",
    "import pickle as pkl\n",
    "dict_accuracy_al, f1_score_list , dict_accuracy_sl , dict_f1_score_sl\n",
    "pkl.dump( dict_accuracy_al, open( \"../../results/sem_tfidf_dict_accuracy_al.p\", \"wb\" ) )\n",
    "pkl.dump( f1_score_list, open( \"../../results/sem_tfidf_score_list.p\", \"wb\" ) )\n",
    "pkl.dump( dict_accuracy_sl, open( \"../../results/sem_tfidf_dict_accuracy_sl.p\", \"wb\" ) )\n",
    "pkl.dump( dict_f1_score_sl, open( \"../../results/sem_tfidf_dict_f1_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( t1_score_list, open( \"../../results/sem_tfidf_dict_t1_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( t2_score_list, open( \"../../results/sem_tfidf_dict_t2_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( t3_score_list, open( \"../../results/sem_tfidf_dict_t3_score_al.p\", \"wb\" ) )\n",
    "pkl.dump( test1_sl, open( \"../../results/sem_tfidf_dict_t1_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( test2_sl, open( \"../../results/sem_tfidf_dict_t2_score_sl.p\", \"wb\" ) )\n",
    "pkl.dump( test3_sl, open( \"../../results/sem_tfidf_dict_t3_score_sl.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating inputs and labels for BOW\n",
    "CV = CountVectorizer()\n",
    "student_answer_count_vector = CV.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "# visualizing the classes\n",
    "with plt.style.context('seaborn-white'):\n",
    "    pca = PCA(n_components=2).fit_transform(X)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(x=pca[:, 0], y=pca[:, 1], c=Y, cmap='viridis', s=50)\n",
    "    plt.title('Semeval dataset -  BOW')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating inputs and labels for TFidf\n",
    "Tf = TfidfVectorizer()\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "# visualizing the classes\n",
    "with plt.style.context('seaborn-white'):\n",
    "    pca = PCA(n_components=3).fit_transform(X)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(x=pca[:, 0], y=pca[:, 1], c=Y, cmap='viridis', s=50)\n",
    "    plt.title('Semeval dataset -  TFIDF')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlgeval",
   "language": "python",
   "name": "maluuba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
