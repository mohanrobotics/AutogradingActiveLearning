{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification SemEVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling,margin_sampling,entropy_sampling\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised_learner():\n",
    "    def __init__(self,X,Y,sp_model):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.model = sp_model\n",
    "\n",
    "    def learn(self):\n",
    "        X_train,X_test,Y_train,Y_test = train_test_split(self.X,self.Y,test_size = 0.2)\n",
    "        model = self.model\n",
    "        model.fit(X_train, Y_train)\n",
    "        model_accuracy = model.score(X_test, Y_test)\n",
    "        model_pred = model.predict(X_test)\n",
    "        model_f1 = f1_score(Y_test,model_pred,average =\"weighted\")\n",
    "        return model_accuracy,model_f1,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Active_learner():\n",
    "    def __init__(self,X,Y,model,data, percentage,query_method):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.short_df = data.copy()\n",
    "        self.percent = percentage\n",
    "        self.model = model\n",
    "        self.query_method = query_method\n",
    "        \n",
    "    def learn(self):       \n",
    "        # seeding\n",
    "        classes = self.short_df['grades_round'].unique()\n",
    "        seed_index = []\n",
    "        for i in classes:\n",
    "            seed_index.append(self.short_df['grades_round'][self.short_df['grades_round']==i].index[0])\n",
    "        seed_index\n",
    "\n",
    "        act_data = self.short_df.copy()\n",
    "        accuracy_list = []\n",
    "\n",
    "        # initialising\n",
    "        train_idx = seed_index\n",
    "        X_train = self.X[train_idx]\n",
    "        y_train = self.Y[train_idx]\n",
    "\n",
    "        # generating the pool\n",
    "        X_pool = np.delete(self.X, train_idx, axis=0)\n",
    "        y_pool = np.delete(self.Y, train_idx)\n",
    "\n",
    "        act_data = act_data.drop(axis=0,index = train_idx)\n",
    "        act_data.reset_index(drop = True,inplace=True)\n",
    "\n",
    "\n",
    "        # initializing the active learner\n",
    "\n",
    "        learner = ActiveLearner(\n",
    "            estimator = self.model,\n",
    "            X_training = X_train, y_training=y_train,\n",
    "            query_strategy=self.query_method\n",
    "        )\n",
    "\n",
    "        # pool-based sampling\n",
    "        n_queries = int(len(X)/(100/self.percent))\n",
    "        for idx in range(n_queries):\n",
    "            query_idx, query_instance = learner.query(X_pool)   \n",
    "            learner.teach(\n",
    "                X=X_pool[query_idx].reshape(1, -1),\n",
    "                y=y_pool[query_idx].reshape(1, )\n",
    "            )\n",
    "\n",
    "            # remove queried instance from pool\n",
    "            X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "            y_pool = np.delete(y_pool, query_idx)\n",
    "\n",
    "            act_data = act_data.drop(axis=0,index = query_idx)\n",
    "            act_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            accuracy_list.append(learner.score(X_pool,y_pool))\n",
    "#             print('Accuracy after query no. %d: %f' % (idx+1, learner.score(X_pool, y_pool)))\n",
    "        print(\"By just labelling \",round(n_queries*100.0/len(X),2),\"% of total data accuracy of \", round(learner.score(X_pool, y_pool),3), \" % is achieved on the unseen data\" )\n",
    "        learner_pred = learner.predict(X_pool)\n",
    "        learner_f1 = f1_score(y_pool,learner_pred,average='weighted')\n",
    "        return accuracy_list,model_f1,learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sultan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating inputs and labels for sultan\n",
    "X = df[['length_ratio','aligned_score','aligned_score_demo','cos_similarity','cos_similarity_demo']]\n",
    "X = np.array(X)\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = []\n",
    "for i,model in enumerate(models):\n",
    "    ac = Active_learner(X,Y,model,df, 30)\n",
    "    accuracy_list,f1 = ac.learn()\n",
    "    dict_accuracy_al[i] = accuracy_list\n",
    "    f1_score_list.append(f1_score)\n",
    "    \n",
    "dict_accuracy_sl= []    \n",
    "#Logistic regression\n",
    "lr_accuracy_list = []\n",
    "lr_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, Y_train)\n",
    "    lr_accuracy = lr.score(X_test, Y_test)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_f1 = f1_score(Y_test,lr_pred,average='weighted')\n",
    "    lr_accuracy_list.append(lr_accuracy)\n",
    "    lr_f1_list.append(lr_f1)\n",
    "lr_accuracy_mean = np.mean(lr_accuracy_list)\n",
    "lr_f1_mean = np.mean(lr_f1_list)\n",
    "dict_accuracy_sl.append(lr_accuracy_mean)\n",
    "\n",
    "# naive bayes classfier    \n",
    "nb_accuracy_list = []\n",
    "nb_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, Y_train)\n",
    "    nb_accuracy = nb.score(X_test, Y_test)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_f1 = f1_score(Y_test,nb_pred,average='weighted')\n",
    "    nb_accuracy_list.append(nb_accuracy)\n",
    "    nb_f1_list.append(nb_f1)\n",
    "nb_accuracy_mean = np.mean(nb_accuracy_list)\n",
    "nb_f1_mean = np.mean(nb_f1_list)\n",
    "dict_accuracy_sl.append(nb_accuracy_mean)\n",
    "\n",
    "\n",
    "\n",
    "# randomforest classfier    \n",
    "rf_accuracy_list = []\n",
    "rf_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, Y_train)\n",
    "    rf_accuracy = rf.score(X_test, Y_test)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_f1 = f1_score(Y_test,rf_pred,average='weighted')\n",
    "    rf_accuracy_list.append(rf_accuracy)\n",
    "    rf_f1_list.append(rf_f1)\n",
    "rf_accuracy_mean = np.mean(rf_accuracy_list)\n",
    "rf_f1_mean = np.mean(rf_f1_list)\n",
    "dict_accuracy_sl.append(rf_accuracy_mean)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC classfier    \n",
    "lsvc_accuracy_list = []\n",
    "lsvc_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    lsvc = SVC(kernel='linear',probability=True)\n",
    "    lsvc.fit(X_train, Y_train)\n",
    "    lsvc_accuracy = lsvc.score(X_test, Y_test)\n",
    "    lsvc_pred = lsvc.predict(X_test)\n",
    "    lsvc_f1 = f1_score(Y_test,lsvc_pred,average='weighted')\n",
    "    lsvc_accuracy_list.append(lsvc_accuracy)\n",
    "    lsvc_f1_list.append(lsvc_f1)\n",
    "lsvc_accuracy_mean = np.mean(lsvc_accuracy_list)\n",
    "lsvc_f1_mean = np.mean(lsvc_f1_list)\n",
    "dict_accuracy_sl.append(lsvc_accuracy_mean)\n",
    "\n",
    "\n",
    "# RBF_SVC   \n",
    "rsvc_accuracy_list = []\n",
    "rsvc_f1_list = []\n",
    "for _ in range(1):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    rsvc = SVC(probability=True)\n",
    "    rsvc.fit(X_train, Y_train)\n",
    "    rsvc_accuracy = rsvc.score(X_test, Y_test)\n",
    "    rsvc_pred = rsvc.predict(X_test)\n",
    "    rsvc_f1 = f1_score(Y_test,rsvc_pred,average='weighted')\n",
    "    rsvc_accuracy_list.append(rsvc_accuracy)\n",
    "    rsvc_f1_list.append(rsvc_f1)\n",
    "rsvc_accuracy_mean = np.mean(rsvc_accuracy_list)\n",
    "rsvc_f1_mean = np.mean(rsvc_f1_list)\n",
    "dict_accuracy_sl.append(rsvc_accuracy_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,20))\n",
    "\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0])-1, len(dict_accuracy_al[0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0]))])\n",
    "    ax.plot(dict_accuracy_al[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8a55e2edff1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActive_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPercent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0maccuracy_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_al\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mdict_accuracy_al\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mf1_score_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-5f9b3540d4fa>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mact_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print('Accuracy after query no. %d: %f' % (idx+1, learner.score(X_pool, y_pool)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"By just labelling \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_queries\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"% of total data accuracy of \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" % is achieved on the unseen data\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maluuba/lib/python3.6/site-packages/modAL/models.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, **score_kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mmean_accuracy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmean\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maluuba/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maluuba/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maluuba/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 308\u001b[0;31m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maluuba/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#creating inputs and labels for BOW\n",
    "CV = CountVectorizer()\n",
    "student_answer_count_vector = CV.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "iteration_count = 2\n",
    "Percent = 30\n",
    "\n",
    "\n",
    "#test\n",
    "test1df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test1.pkl\")\n",
    "student_answer_count_vector = CV.fit_transform(test1df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test1X = student_answer_count_vector\n",
    "test1Y = df['grades_round'].values\n",
    "test2df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test2.pkl\")\n",
    "student_answer_count_vector = CV.fit_transform(test2df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test2X = student_answer_count_vector\n",
    "test2Y = df['grades_round'].values\n",
    "test3df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test3.pkl\")\n",
    "student_answer_count_vector = CV.fit_transform(test3df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test3X = student_answer_count_vector\n",
    "test3Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Active learner\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "query_methods = [uncertainty_sampling,margin_sampling,entropy_sampling]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = {}\n",
    "t1_score_list = {}\n",
    "t2_score_list = {}\n",
    "t3_score_list = {}\n",
    "for i,model in enumerate(models):\n",
    "    dict_accuracy_al[i] = []\n",
    "    f1_score_list[i] = []\n",
    "    t1_score_list[i] = []\n",
    "    t2_score_list[i] = []\n",
    "    t3_score_list[i] = []\n",
    "    print(\"******************************************************************************\")\n",
    "    for query_method in query_methods:\n",
    "        ac = Active_learner(X,Y,model,df, Percent,query_method)\n",
    "        accuracy_list,f1,model_al = ac.learn()\n",
    "        dict_accuracy_al[i].append(accuracy_list)\n",
    "        f1_score_list[i].append(f1_score)\n",
    "        t1_score_list[i].append(model_al.score(test1X,test1Y))\n",
    "        t2_score_list[i].append(model_al.score(test2X,test2Y))\n",
    "        t3_score_list[i].append(model_al.score(test3X,test3Y))\n",
    "\n",
    "        \n",
    "## Supervised learner\n",
    "dict_accuracy_sl= []\n",
    "dict_f1_score_sl= []\n",
    "test1_sl = []\n",
    "test2_sl = []\n",
    "test3_sl = []\n",
    "\n",
    "for model in models:\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    test3 = []\n",
    "    for _ in range(iteration_count):\n",
    "        sl = Supervised_learner(X,Y,model)\n",
    "        accuracy,f1score,model_sl = sl.learn()\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1score)\n",
    "        test1.append(model_sl.score(test1X,test1Y))\n",
    "        test2.append(model_sl.score(test2X,test2Y))\n",
    "        test3.append(model_sl.score(test3X,test3Y))\n",
    "        \n",
    "        \n",
    "    dict_accuracy_sl.append(np.mean(accuracy_list))\n",
    "    dict_f1_score_sl.append(np.mean(f1_list))    \n",
    "    test1_sl.append(np.mean(test1))\n",
    "    test2_sl.append(np.mean(test2))\n",
    "    test3_sl.append(np.mean(test3))\n",
    "    \n",
    "    \n",
    "## plotting\n",
    "fig = plt.figure(figsize=(18,20))\n",
    "model_title = [\"Logistic regression\",\"Naive Bayes\",\"Random Forest\",\"SVC-linear\",\"SVC\"]\n",
    "query_strategy = [\"uncertainty_sampling\",\"margin_sampling\",\"entropy_sampling\"]\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0][0])-1, len(dict_accuracy_al[0][0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0][0]))],label = \"Supervised learner\")\n",
    "    for j in range(len(query_strategy)):\n",
    "        ax.plot(dict_accuracy_al[i][j],label = \"Active learner {}\".format(query_strategy[j]))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel(\"Number of queries\")\n",
    "    ax.set_ylabel(\"Accuracy %\")\n",
    "    ax.set_title(model_title[i])\n",
    "    ax.grid(color='g', linestyle='-', linewidth=0.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## storing the pikle\n",
    "import pickle as pkl\n",
    "dict_accuracy_al, f1_score_list , dict_accuracy_sl , dict_f1_score_sl\n",
    "pkl.dump( dict_accuracy_al, open( \"../../results/sem_bag_dict_accuracy_al.p\", \"wb\" ) )\n",
    "pkl.dump( f1_score_list, open( \"../../results/sem_bag_tfidf_score_list.p\", \"wb\" ) )\n",
    "pkl.dump( dict_accuracy_sl, open( \"../../results/sem_bag_dict_accuracy_sl.p\", \"wb\" ) )\n",
    "pkl.dump( dict_f1_score_sl, open( \"../../results/sem_bag_dict_f1_score_sl.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-IDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating inputs and labels for TFidf\n",
    "Tf = TfidfVectorizer()\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "iteration_count = 2\n",
    "Percent = 30\n",
    "\n",
    "\n",
    "#test\n",
    "test1df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test1.pkl\")\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test1X = student_answer_count_vector\n",
    "test1Y = df['grades_round'].values\n",
    "\n",
    "test2df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test2.pkl\")\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test2X = student_answer_count_vector\n",
    "test2Y = df['grades_round'].values\n",
    "\n",
    "test3df = pd.read_pickle(\"../../dataset/final_dataset/sem_final_test3.pkl\")\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "test3X = student_answer_count_vector\n",
    "test3Y = df['grades_round'].values\n",
    "\n",
    "## Active learner\n",
    "models = [LogisticRegression(),MultinomialNB(),RandomForestClassifier(),SVC(kernel='linear' , probability=True),SVC(probability=True)]\n",
    "query_methods = [uncertainty_sampling,margin_sampling,entropy_sampling]\n",
    "dict_accuracy_al ={}\n",
    "f1_score_list = {}\n",
    "t1_score_list = {}\n",
    "t2_score_list = {}\n",
    "t3_score_list = {}\n",
    "for i,model in enumerate(models):\n",
    "    dict_accuracy_al[i] = []\n",
    "    f1_score_list[i] = []\n",
    "    t1_score_list[i] = []\n",
    "    t2_score_list[i] = []\n",
    "    t3_score_list[i] = []\n",
    "    print(\"******************************************************************************\")\n",
    "    for query_method in query_methods:\n",
    "        ac = Active_learner(X,Y,model,df, Percent,query_method)\n",
    "        accuracy_list,f1,model_al = ac.learn()\n",
    "        dict_accuracy_al[i].append(accuracy_list)\n",
    "        f1_score_list[i].append(f1_score)\n",
    "        t1_score_list[i].append(model_al.score(test1X,test1Y))\n",
    "        t2_score_list[i].append(model_al.score(test2X,test2Y))\n",
    "        t3_score_list[i].append(model_al.score(test3X,test3Y))\n",
    "\n",
    "        \n",
    "## Supervised learner\n",
    "dict_accuracy_sl= []\n",
    "dict_f1_score_sl= []\n",
    "test1_sl = []\n",
    "test2_sl = []\n",
    "test3_sl = []\n",
    "\n",
    "for model in models:\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    test3 = []\n",
    "    for _ in range(iteration_count):\n",
    "        sl = Supervised_learner(X,Y,model)\n",
    "        accuracy,f1score,model_sl = sl.learn()\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1score)\n",
    "        test1.append(model_sl.score(test1X,test1Y))\n",
    "        test2.append(model_sl.score(test2X,test2Y))\n",
    "        test3.append(model_sl.score(test3X,test3Y))\n",
    "        \n",
    "        \n",
    "    dict_accuracy_sl.append(np.mean(accuracy_list))\n",
    "    dict_f1_score_sl.append(np.mean(f1_list))    \n",
    "    test1_sl.append(np.mean(test1))\n",
    "    test2_sl.append(np.mean(test2))\n",
    "    test3_sl.append(np.mean(test3))\n",
    "    \n",
    "    \n",
    "## plotting\n",
    "fig = plt.figure(figsize=(18,20))\n",
    "model_title = [\"Logistic regression\",\"Naive Bayes\",\"Random Forest\",\"SVC-linear\",\"SVC\"]\n",
    "query_strategy = [\"uncertainty_sampling\",\"margin_sampling\",\"entropy_sampling\"]\n",
    "for i in range(0,5):\n",
    "    ax = fig.add_subplot(3,2,i+1)\n",
    "    ax.plot(np.linspace(0,len(dict_accuracy_al[0][0])-1, len(dict_accuracy_al[0][0]) ), \\\n",
    "            [dict_accuracy_sl[i] for _ in range(len(dict_accuracy_al[0][0]))],label = \"Supervised learner\")\n",
    "    for j in range(len(query_strategy)):\n",
    "        ax.plot(dict_accuracy_al[i][j],label = \"Active learner {}\".format(query_strategy[j]))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel(\"Number of queries\")\n",
    "    ax.set_ylabel(\"Accuracy %\")\n",
    "    ax.set_title(model_title[i])\n",
    "    ax.grid(color='g', linestyle='-', linewidth=0.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## storing the pikle\n",
    "import pickle as pkl\n",
    "dict_accuracy_al, f1_score_list , dict_accuracy_sl , dict_f1_score_sl\n",
    "pkl.dump( dict_accuracy_al, open( \"../../results/sem_tfidf_dict_accuracy_al.p\", \"wb\" ) )\n",
    "pkl.dump( f1_score_list, open( \"../../results/sem_bag_tfidf_score_list.p\", \"wb\" ) )\n",
    "pkl.dump( dict_accuracy_sl, open( \"../../results/sem_tfidf_dict_accuracy_sl.p\", \"wb\" ) )\n",
    "pkl.dump( dict_f1_score_sl, open( \"../../results/sem_tfidf_dict_f1_score_sl.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating inputs and labels for BOW\n",
    "CV = CountVectorizer()\n",
    "student_answer_count_vector = CV.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "# visualizing the classes\n",
    "with plt.style.context('seaborn-white'):\n",
    "    pca = PCA(n_components=2).fit_transform(X)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(x=pca[:, 0], y=pca[:, 1], c=Y, cmap='viridis', s=50)\n",
    "    plt.title('Semeval dataset -  BOW')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating inputs and labels for TFidf\n",
    "Tf = TfidfVectorizer()\n",
    "student_answer_count_vector = Tf.fit_transform(df['student_modified'])\n",
    "student_answer_count_vector = student_answer_count_vector.toarray()\n",
    "\n",
    "X = student_answer_count_vector\n",
    "Y = df['grades_round'].values\n",
    "\n",
    "\n",
    "# visualizing the classes\n",
    "with plt.style.context('seaborn-white'):\n",
    "    pca = PCA(n_components=3).fit_transform(X)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(x=pca[:, 0], y=pca[:, 1], c=Y, cmap='viridis', s=50)\n",
    "    plt.title('Semeval dataset -  TFIDF')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlgeval",
   "language": "python",
   "name": "maluuba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
