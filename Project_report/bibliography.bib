@article{Bachman2002,
abstract = {This paper discusses an innovative approach to the computer assisted scoring of student responses in WebLAS (web-based language assessment system)- a language assessment system delivered entirely over the web. Expected student responses are limited production free response questions. The portions of WebLAS with which we are concerned are the task creation and scoring modules. Within the task creation module, instructors and language experts do not only provide the task input and prompt. More importantly, they interactively inform the system how and how much to score student responses. This interaction consists of WebLAS' natural language processing (NLP) modules searching for alternatives of the provided “gold standard” (Hirschman et al, 2000) answer and asking for confirmation of score assignment. WebLAS processes and stores all this information within its database, to be used in the task delivery and scoring phases.},
author = {Bachman, Lyle F and Carr, Nathan and Kamei, Greg and Kim, Mikyung and Pan, Michael J and Salvador, Chris and Sawaki, Yasuyo},
doi = {10.3115/1071884.1071907},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/A reliable approach to automatic assessment
of short answer free responses.pdf:pdf},
journal = {Proceedings of the 19th international conference on Computational linguistics -},
pages = {1--4},
title = {{A reliable approach to automatic assessment of short answer free responses}},
volume = {2},
year = {2002}
}
@article{Settles2010,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
eprint = {1206.5533},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Active Learning Literature Survey.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {00483931},
journal = {Machine Learning},
number = {2},
pages = {201--221},
pmid = {15003161},
title = {{Active Learning Literature Survey}},
volume = {15},
year = {2010}
}
@article{Brill2002,
abstract = {We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong an-swer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.},
author = {Brill, Eric and Dumais, Susan and Banko, Michele},
doi = {10.3115/1118693.1118726},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/An Analysis of the AskMSR Question-Answering System.pdf:pdf},
journal = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP '02},
number = {July},
pages = {257--264},
title = {{An analysis of the AskMSR question-answering system}},
volume = {10},
year = {2002}
}
@article{Wang2008,
abstract = {The work aims to improve the assessment of creative problem-solving in science education by employing language technologies and computational-statistical machine learning methods to grade students' natural language responses automatically. To evaluate constructs like creative problem-solving with validity, open-ended questions that elicit students' constructed responses are beneficial. But the high cost required in manually grading constructed responses could become an obstacle in applying open-ended questions. In this study, automated grading schemes have been developed and evaluated in the context of secondary Earth science education. Empirical evaluations revealed that the automated grading schemes may reliably identify domain concepts embedded in students' natural language responses with satisfactory inter-coder agreement against human coding in two sub-tasks of the test (Cohen's Kappa = .65-.72). And when a single holistic score was computed for each student, machine-generated scores achieved high inter-rater reliability against human grading (Pearson's r = .92). The reliable performance in automatic concept identification and numeric grading demonstrates the potential of using automated grading to support the use of open-ended questions in science assessments and enable new technologies for science learning. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Wang, Hao Chuan and Chang, Chun Yen and Li, Tsai Yen},
doi = {10.1016/j.compedu.2008.01.006},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Assessing creative problem-solving with automated text grading.pdf:pdf},
isbn = {0360-1315},
issn = {03601315},
journal = {Computers and Education},
keywords = {Automated grading,Computer-aided assessment,Creative problem-solving,Machine learning application,Science learning assessment},
number = {4},
pages = {1450--1466},
title = {{Assessing creative problem-solving with automated text grading}},
volume = {51},
year = {2008}
}
@article{Pulman2005,
abstract = {Our aim is to investigate computational linguistics (CL) techniques in marking short free text responses automatically. Successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding. However, recent advances in CL techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers. This paper describes some of the techniques we have tried so far vis-{\`{a}}-vis this problem with results, discussion and description of the main issues encountered},
author = {Pulman, Stephen G. and Sukkarieh, Jana Z.},
doi = {10.3115/1609829.1609831},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Automatic short answer marking - Sukkarieh.pdf:pdf},
journal = {EdAppsNLP 05 Proceedings of the second workshop on Building Educational Applications Using NLP},
number = {June},
pages = {9--16},
title = {{Automatic short answer marking}},
year = {2005}
}
@article{Leacock2003,
abstract = {C-rater is an automated scoring engine that has been developed to score responses to content-based short answer questions. It is not simply a string matching program – instead it uses predicate argument structure, pronominal reference, morphological analysis and synonyms to assign full or partial credit to a short answer question. C-rater has been used in two studies: National Assessment for Educational Progress (NAEP) and a statewide assessment in Indiana. In both studies, c-rater agreed with human graders about 84{\%} of the time.},
author = {Leacock, Claudia and Chodorow, Martin},
doi = {10.1023/A:1025779619903},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/C-rater$\backslash$: Automated Scoring of Short-Answer Questions.pdf:pdf},
isbn = {0010-4817},
issn = {00104817},
journal = {Computers and the Humanities},
keywords = {Automated scoring,Content-based scoring,Short answer scoring},
number = {4},
pages = {389--405},
title = {{C-rater: Automated scoring of short-answer questions}},
volume = {37},
year = {2003}
}
@article{Higgins2004,
abstract = {CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements). We describe a new system that enhances Criterion's capability, by evaluating multiple aspects of coherence in essays. This system identifies features of sentences based on semantic similarity measures and discourse structure. A support vector machine uses these features to capture breakdowns in coherence due to relatedness between discourse elements. Intra-sentential quality is evaluated with rule-based heuristics. Results indicate that the system yields higher performance than a baseline on all three aspects.},
author = {Higgins, Derrick and Burstein, Jill and Marcu, Daniel},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Evaluating Multiple Aspects of Coherence in Student Essays.pdf:pdf},
journal = {Hlt-Naacl},
pages = {185--192},
title = {{Evaluating Multiple Aspects of Coherence in Student Essays}},
year = {2004}
}
@article{Sultan2016,
abstract = {We present a fast, simple, and high-accuracy short answer grading system. Given a short-answer question and its correct answer, key measures of the correctness of a student re-sponse can be derived from its semantic sim-ilarity with the correct answer. Our super-vised model (1) utilizes recent advances in the identification of short-text similarity, and (2) augments text similarity features with key grading-specific constructs. We present exper-imental results where our model demonstrates top performance on multiple benchmarks.},
author = {Sultan, Md Arafat and Salazar, Cristobal and Sumner, Tamara},
doi = {10.18653/v1/N16-1123},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Fast and Easy Short Answer Grading with High Accuracy.pdf:pdf},
isbn = {9781941643914},
journal = {Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1070--1075},
title = {{Fast and Easy Short Answer Grading with High Accuracy}},
year = {2016}
}
@article{Ramachandran2015,
abstract = {Short answer scoring systems typically use regular expressions, templates or logic expres-sions to detect the presence of specific terms or concepts among student responses. Pre-vious work has shown that manually devel-oped regular expressions can provide effective scoring, however manual development can be quite time consuming. In this work we present a new approach that uses word-order graphs to identify important patterns from human-provided rubric texts and top-scoring student answers. The approach also uses semantic metrics to determine groups of related words, which can represent alternative answers. We evaluate our approach on two datasets: (1) the Kaggle Short Answer dataset (ASAP-SAS, 2012), and (2) a short answer dataset provided by Mohler et al. (2011). We show that our automated approach performs better than the best performing Kaggle entry and generalizes as a method to the Mohler dataset.},
author = {Ramachandran, Lakshmi and Cheng, Jian and Foltz, Peter},
doi = {10.3115/v1/W15-0612},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Identifying Patterns For Short Answer Scoring
Using Graph-based Lexico-Semantic Text Matching.pdf:pdf},
journal = {Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications},
pages = {97--106},
title = {{Identifying Patterns For Short Answer Scoring Using Graph-based Lexico-Semantic Text Matching}},
year = {2015}
}
@article{Horbach2016,
abstract = {Active learning has been shown to be effec-tive for reducing human labeling effort in su-pervised learning tasks, and in this work we explore its suitability for automatic short an-swer assessment on the ASAP corpus. We systematically investigate a wide range of AL settings, varying not only the item selection method but also size and selection of seed set items and batch size. Comparing to a random baseline and a recently-proposed diversity-based baseline which uses cluster centroids as training data, we find that uncertainty-based sampling methods can be beneficial, espe-cially for data sets with particular properties. The performance of AL, however, varies con-siderably across individual prompts.},
author = {Horbach, Andrea and Palmer, Alexis and Sciencecampus, Leibniz},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Investigating Active Learning for Short-Answer Scoring:},
pages = {301--311},
title = {{Investigating Active Learning for Short-Answer Scoring}},
year = {2016}
}
@article{Higgins2014,
abstract = {Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach.},
archivePrefix = {arXiv},
arxivId = {1403.0801},
author = {Higgins, Derrick and Brew, Chris and Heilman, Michael and Ziai, Ramon and Chen, Lei and Cahill, Aoife and Flor, Michael and Madnani, Nitin and Tetreault, Joel and Blanchard, Daniel and Napolitano, Diane and Lee, Chong Min and Blackmore, John},
eprint = {1403.0801},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Is getting the right answer just about choosing the right words? The role of
syntactically-informed features in short answer scoring..pdf:pdf},
title = {{Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring}},
year = {2014}
}
@article{Mohler2011,
author = {Mohler, M. and Bunescu, R. and Mihalcea, R.},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments - Mohler.pdf:pdf},
pages = {752--762},
title = {{Learning to grade short answer questions using semantic similarity measures and dependency graph alignments}},
year = {2011}
}
@article{Mohler2009,
abstract = {In this paper, we explore unsupervised techniques for the task of automatic short answer grading. We compare a number of knowledge-based and corpus-based measures of text similarity, evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers. Overall, our system significantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past.},
author = {Mohler, Michael and Mihalcea, Rada},
doi = {10.3115/1609067.1609130},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Text-to-text Semantic Similarity for Automatic Short Answer Grading - Mohler.pdf:pdf},
isbn = {9781932432169},
journal = {Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL '09)},
number = {April},
pages = {567--575},
title = {{Text-to-text Semantic Similarity for Automatic Short Answer Grading}},
year = {2009}
}
@book{Burrows2015,
abstract = {Abstract Automatic short answer grading (ASAG) is the task of assessing short nat- ural language responses to objective questions using computational methods. The active research in this field has increased enormously of late with over 80 papers fit- ting a definition of ASAG. However, the past efforts have generally been ad-hoc and non-comparable until recently, hence the need for a unified view of the whole field. The goal of this paper is to address this aim with a comprehensive review of ASAG research and systems according to history and components. Our historical analysis identifies 35 ASAG systems within 5 temporal themes that mark advancement in methodology or evaluation. In contrast, our component analysis reviews 6 common dimensions from preprocessing to effectiveness. A key conclusion is that an era of evaluation is the newest trend in ASAG research, which is paving the way for the consolidation of the field.},
author = {Burrows, Steven and Gurevych, Iryna and Stein, Benno},
booktitle = {International Journal of Artificial Intelligence in Education},
doi = {10.1007/s40593-014-0026-8},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/The Eras and Trends of Automatic Short Answer Grading.pdf:pdf},
isbn = {4059301400268},
issn = {15604306},
keywords = {Automatic grading,Natural language processing,Short answer},
number = {1},
pages = {60--117},
title = {{The eras and trends of automatic short answer grading}},
volume = {25},
year = {2015}
}
@article{Roy2016a,
author = {Roy, Shourya and Dandapat, Sandipan and Nagesh, Ajay and Y., Narahari},
file = {:home/black-book/HBRS/II/RnD/NLP Papers/Wisdom of Students$\backslash$: A Consistent Automatic
Short Answer Grading Technique:},
journal = {Proceedings of the 13th International Conference on Natural Language Processing},
pages = {178--187},
title = {{Wisdom of Students: A Consistent Automatic Short Answer Grading Technique}},
year = {2016}
}
@article{Mitchell2002,
  title={Towards robust computerised marking of free-text responses},
  author={Mitchell, Tom and Russell, Terry and Broomhead, Peter and Aldridge, Nicola},
  year={2002},
  publisher={{\copyright} Loughborough University}
}
@inproceedings{Mihalcea2006,
  title={Corpus-based and knowledge-based measures of text semantic similarity},
  author={Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo and others},
  booktitle={AAAI},
  volume={6},
  pages={775--780},
  year={2006}
}
@article{Jordan2009,
  title={e-Assessment for learning? The potential of short-answer free-text questions with tailored feedback},
  author={Jordan, Sally and Mitchell, Tom},
  journal={British Journal of Educational Technology},
  volume={40},
  number={2},
  pages={371--385},
  year={2009},
  publisher={Wiley Online Library}
}
@article{Nielsen2009,
  title={Recognizing entailment in intelligent tutoring systems},
  author={Nielsen, Rodney D and Ward, Wayne and Martin, James H},
  journal={Natural Language Engineering},
  volume={15},
  number={4},
  pages={479--501},
  year={2009},
  publisher={Cambridge University Press}
}
@inproceedings{Meurers2011,
  title={Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure},
  author={Meurers, Detmar and Ziai, Ramon and Ott, Niels and Kopp, Janina},
  booktitle={Proceedings of the TextInfer 2011 Workshop on Textual Entailment},
  pages={1--9},
  year={2011},
  organization={Association for Computational Linguistics}
}
@inproceedings{Hahn2012,
  title={Evaluating the meaning of answers to reading comprehension questions a semantics-based approach},
  author={Hahn, Michael and Meurers, Detmar},
  booktitle={Proceedings of the Seventh Workshop on Building Educational Applications Using NLP},
  pages={326--336},
  year={2012},
  organization={Association for Computational Linguistics}
}
@article{Siddiqi2010,
  title={Improving teaching and learning through automated short-answer marking},
  author={Siddiqi, Raheel and Harrison, Christopher J and Siddiqi, Rosheena},
  journal={IEEE Transactions on Learning Technologies},
  volume={3},
  number={3},
  pages={237--249},
  year={2010},
  publisher={IEEE}
}
@article{Gomaa2012,
  title={Short answer grading using string similarity and corpus-based similarity},
  author={Gomaa, Wael H and Fahmy, Aly A},
  journal={International Journal of Advanced Computer Science and Applications (IJACSA)},
  volume={3},
  number={11},
  year={2012},
  publisher={Citeseer}
}
@article{Basu2013,
  title={Powergrading: a clustering approach to amplify human effort for short answer grading},
  author={Basu, Sumit and Jacobs, Chuck and Vanderwende, Lucy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={1},
  pages={391--402},
  year={2013}
}
@article{Higgins2014,
  title={Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring},
  author={Higgins, Derrick and Brew, Chris and Heilman, Michael and Ziai, Ramon and Chen, Lei and Cahill, Aoife and Flor, Michael and Madnani, Nitin and Tetreault, Joel and Blanchard, Daniel and others},
  journal={arXiv preprint arXiv:1403.0801},
  year={2014}
}
@article{Callear2001,
  title={CAA of short non-MCQ answers},
  author={Callear, David and Jerrams-Smith, Jenny and Soh, Victor},
  year={2001},
  publisher={{\copyright} Loughborough University}
}
@inproceedings{zesch2015,
  title={Reducing annotation efforts in supervised short answer scoring},
  author={Zesch, Torsten and Heilman, Michael and Cahill, Aoife},
  booktitle={Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={124--132},
  year={2015}
}
@inproceedings{dligach2011,
  title={Good seed makes a good crop: accelerating active learning using language modeling},
  author={Dligach, Dmitriy and Palmer, Martha},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2},
  pages={6--10},
  year={2011},
  organization={Association for Computational Linguistics}
}
@article{figueroa2012,
  title={Active learning for clinical text classification: is it better than random sampling?},
  author={Figueroa, Rosa L and Zeng-Treitler, Qing and Ngo, Long H and Goryachev, Sergey and Wiechmann, Eduardo P},
  journal={Journal of the American Medical Informatics Association},
  volume={19},
  number={5},
  pages={809--816},
  year={2012},
  publisher={BMJ Group BMA House, Tavistock Square, London, WC1H 9JR}
}
@inproceedings{mccallumzy1998,
  title={Employing EM and pool-based active learning for text classification},
  author={McCallumzy, Andrew Kachites and Nigamy, Kamal},
  booktitle={Proc. International Conference on Machine Learning (ICML)},
  pages={359--367},
  year={1998},
  organization={Citeseer}
}
@article{tong2001,
  title={Support vector machine active learning with applications to text classification},
  author={Tong, Simon and Koller, Daphne},
  journal={Journal of machine learning research},
  volume={2},
  number={Nov},
  pages={45--66},
  year={2001}
}
@techreport{dzikovska2013,
  title={Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge},
  author={Dzikovska, Myroslava O and Nielsen, Rodney D and Brew, Chris and Leacock, Claudia and Giampiccolo, Danilo and Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Dang, Hoa T},
  year={2013},
  institution={NORTH TEXAS STATE UNIV DENTON}
}
@misc{kaggle,
  title = {SimEval},
  howpublished = {\url{https://en.wikipedia.org/wiki/SemEval}},
  note = {Accessed: 2018-05-27}
}
@article{mason2002,
  title={Automated free text marking with paperless school},
  author={Mason, Oliver and Grove-Stephensen, Ian},
  year={2002},
  publisher={{\copyright} Loughborough University}
}
@inproceedings{yang2009effective,
  title={Effective multi-label active learning for text classification},
  author={Yang, Bishan and Sun, Jian-Tao and Wang, Tengjiao and Chen, Zheng},
  booktitle={Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={917--926},
  year={2009},
  organization={ACM}
}
@inproceedings{nielsen2008annotating,
  title={Annotating Students' Understanding of Science Concepts.},
  author={Nielsen, Rodney D and Ward, Wayne H and Martin, James H and Palmer, Martha},
  booktitle={LREC},
  year={2008}
}
@techreport{de2008stanford,
  title={Stanford typed dependencies manual},
  author={De Marneffe, Marie-Catherine and Manning, Christopher D},
  year={2008},
  institution={Technical report, Stanford University}
}
@misc{nlp_appli,
  title = {Applications of Natural Language Processing},
  howpublished = {\url{https://www.lifewire.com/applications-of-natural-language-processing-technology-2495544}},
  note = {Accessed: 2018-08-27}
}
@article{goldberg2017neural,
  title={Neural network methods for natural language processing},
  author={Goldberg, Yoav},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={10},
  number={1},
  pages={1--309},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}
@book{jurafsky2014speech,
  title={Speech and language processing},
  author={Jurafsky, Dan and Martin, James H},
  volume={3},
  year={2014},
  publisher={Pearson London}
}
@misc{kdnuggets_preprocessing,
  title = {A General Approach to Preprocessing Text Data},
  howpublished = {\url{https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html}},
  note = {Accessed: 2018-08-27}
}
@misc{nltk_list,
  title = {NLTK's list of english stopwords},
  howpublished = {\url{https://gist.github.com/sebleier/554280}},
  note = {Accessed: 2018-08-27}
}
@misc{bow_model,
  title = {A Gentle Introduction to the Bag-of-Words Model},
  howpublished = {\url{https://machinelearningmastery.com/gentle-introduction-bag-words-model/}},
  note = {Accessed: 2018-11-22}
}
@misc{tf_idf_defi,
  title = {tf-idf},
  howpublished = {\url{https://en.wikipedia.org/wiki/Tf%E2%80%93idf}},
  note = {Accessed: 2018-11-22}
}
@misc{tf_idf_1,
  title = {Finding The Most Important Sentences Using NLP and TF-IDF},
  howpublished = {\url{https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3}},
  note = {Accessed: 2018-11-22}
}
@misc{word2vec,
  title = {word2vec},
  howpublished = {\url{https://code.google.com/archive/p/word2vec/}},
  note = {Accessed: 2018-08-27}
}
@misc{fasttext,
  title = {fastText},
  howpublished = {\url{https://research.fb.com/fasttext/}},
  note = {Accessed: 2018-08-27}
}
@misc{glove,
  title = {GloVe: Global Vectors for Word Representation},
  howpublished = {\url{https://nlp.stanford.edu/projects/glove/}},
  note = {Accessed: 2018-08-27}
}
@misc{shane_wordembedding,
  title = {Get Busy with Word Embeddings – An Introduction},
  howpublished = {\url{https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/}},
  note = {Accessed: 2018-08-27}
}
@misc{pos_wiki,
  title = {Part–of–speech tagging},
  howpublished = {\url{https://en.wikipedia.org/wiki/Part-of-speech_tagging}},
  note = {Accessed: 2018-11-22}
}
@misc{ner_blog,
  title = {Named Entity Recognition: Applications and Use Cases},
  howpublished = {\url{https://towardsdatascience.com/named-entity-recognition-applications-and-use-cases-acdbf57d595e}},
  note = {Accessed: 2018-11-22}
}
@misc{dep_visi,
  title = {displaCy Dependency Visualizer},
  howpublished = {\url{https://explosion.ai/demos/displacy}},
  note = {Accessed: 2018-11-22}
}
@misc{2d_embedding,
  title = {Chatbots with Seq2Seq},
  howpublished = {\url{http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/}},
  note = {Accessed: 2018-08-27}
}
@misc{active_learning,
  title = {Active Learning},
  howpublished = {\url{https://en.wikipedia.org/wiki/Active_learning_(machine_learning)}},
  note = {Accessed: 2018-05-28}
}
@misc{active_learning_datacamp,
  title = {Active Learning Curious AI algorithms},
  howpublished = {\url{https://www.datacamp.com/community/tutorials/active-learning}},
  note = {Accessed: 2018-05-28}
}
@incollection{dagan1995committee,
  title={Committee-based sampling for training probabilistic classifiers},
  author={Dagan, Ido and Engelson, Sean P},
  booktitle={Machine Learning Proceedings 1995},
  pages={150--157},
  year={1995},
  publisher={Elsevier}
}
@misc{modal_disagree,
  title = {modAL: A modular active learning framework for Python3},
  howpublished = {\url{https://modal-python.readthedocs.io/en/latest/}},
  note = {Accessed: 2018-11-22}
}
@inproceedings{mccallumzy1998employing,
  title={Employing EM and pool-based active learning for text classification},
  author={McCallumzy, Andrew Kachites and Nigamy, Kamal},
  booktitle={Proc. International Conference on Machine Learning (ICML)},
  pages={359--367},
  year={1998},
  organization={Citeseer}
}
@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}
@inproceedings{settles2008multiple,
  title={Multiple-instance active learning},
  author={Settles, Burr and Craven, Mark and Ray, Soumya},
  booktitle={Advances in neural information processing systems},
  pages={1289--1296},
  year={2008}
}
@article{geman1992neural,
  title={Neural networks and the bias/variance dilemma},
  author={Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal={Neural computation},
  volume={4},
  number={1},
  pages={1--58},
  year={1992},
  publisher={MIT Press}
}
@article{chaloner1995bayesian,
  title={Bayesian experimental design: A review},
  author={Chaloner, Kathryn and Verdinelli, Isabella},
  journal={Statistical Science},
  pages={273--304},
  year={1995},
  publisher={JSTOR}
}
@inproceedings{cohn1995active,
  title={Active learning with statistical models},
  author={Cohn, David A and Ghahramani, Zoubin and Jordan, Michael I},
  booktitle={Advances in neural information processing systems},
  pages={705--712},
  year={1995}
}
@inproceedings{settles2008analysis,
  title={An analysis of active learning strategies for sequence labeling tasks},
  author={Settles, Burr and Craven, Mark},
  booktitle={Proceedings of the conference on empirical methods in natural language processing},
  pages={1070--1079},
  year={2008},
  organization={Association for Computational Linguistics}
}
@article{liu2016validation,
  title={Validation of automated scoring of science assessments},
  author={Liu, Ou Lydia and Rios, Joseph A and Heilman, Michael and Gerard, Libby and Linn, Marcia C},
  journal={Journal of Research in Science Teaching},
  volume={53},
  number={2},
  pages={215--233},
  year={2016},
  publisher={Wiley Online Library}
}
@inproceedings{kumar2017earth,
  title={Earth Mover’s Distance Pooling over Siamese LSTMs for Automatic Short Answer Grading},
  author={Kumar, Sachin and Chakrabarti, Soumen and Roy, Shourya},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  pages={2046--2052},
  year={2017}
}
@misc{cos_sim,
  title = {Overview of Text Similarity Metrics in Python},
  howpublished = {\url{https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50}},
  note = {Accessed: 2018-11-24}
}
@article{sultan2014back,
  title={Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence},
  author={Sultan, Md Arafat and Bethard, Steven and Sumner, Tamara},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={219--230},
  year={2014}
}
@inproceedings{ganitkevitch2013ppdb,
  title={PPDB: The paraphrase database},
  author={Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={758--764},
  year={2013}
}
@misc{spacy,
  title = {spaCy},
  howpublished = {\url{https://spacy.io/}},
  note = {Accessed: 2018-08-27}
}
@misc{nltk,
  title = {Natural Language Toolkit},
  howpublished = {\url{https://www.nltk.org/}},
  note = {Accessed: 2018-08-27}
}
@misc{numpy,
  title = {NumPy},
  howpublished = {\url{http://www.numpy.org/}},
  note = {Accessed: 2018-08-27}
}
@misc{pandas,
  title = {Python Data Analysis Library},
  howpublished = {\url{https://pandas.pydata.org/}},
  note = {Accessed: 2018-08-27}
}
@article{scikit_learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@book{raschka2015python,
  title={Python machine learning},
  author={Raschka, Sebastian},
  year={2015},
  publisher={Packt Publishing Ltd}
}
@incollection{lewis1994heterogeneous,
  title={Heterogeneous uncertainty sampling for supervised learning},
  author={Lewis, David D and Catlett, Jason},
  booktitle={Machine Learning Proceedings 1994},
  pages={148--156},
  year={1994},
  publisher={Elsevier}
}
@inproceedings{scheffer2001active,
  title={Active hidden markov models for information extraction},
  author={Scheffer, Tobias and Decomain, Christian and Wrobel, Stefan},
  booktitle={International Symposium on Intelligent Data Analysis},
  pages={309--318},
  year={2001},
  organization={Springer}
}
@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Wiley Online Library}
}
@article{modAL2018,
    title={mod{AL}: {A} modular active learning framework for {P}ython},
    author={Tivadar Danka and Peter Horvath},
    url={https://github.com/cosmic-cortex/modAL},
    note={available on arXiv at \url{https://arxiv.org/abs/1805.00979}}
}

@unpublished{ramesh2,
	Authors = {Ramesh Kumar},
	Month = {January},
	Note = {WS17
	H-BRS - Evaluation of Semantic Textual Similarity Approaches for Automatic Short Answer Grading Ploeger, Nair supervising},
	Title = {Evaluation of Semantic Textual Similarity Approaches for Automatic Short Answer Grading},
	Year = {2017/18}}

@article{attenberg2013class,
 title={Class imbalance and active learning},
 author={Attenberg, Josh and Ertekin, Seyda},
 journal={H. He, \& Y. Ma, Imbalanced Learning: Foundations, Algorithms, and Applications},
 pages={101--149},
 year={2013},
 publisher={Citeseer}
}

@inproceedings{ertekin2007active,
 title={Active learning for class imbalance problem},
 author={Ertekin, Seyda and Huang, Jian and Giles, C Lee},
 booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
 pages={823--824},
 year={2007},
 organization={ACM}
}